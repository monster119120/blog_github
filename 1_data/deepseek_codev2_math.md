# DeepSeek-Coder-V2 预训练数据处理流程详解：代码与数学领域

## 1. 数据概览与配比
DeepSeek-Coder-V2 的预训练数据由以下三部分组成：
*   **60% 源代码 (Source Code)**
*   **10% 数学语料 (Math Corpus)**
*   **30% 自然语言语料 (Natural Language Corpus)**

## 2. 源代码数据处理 (Source Code)

### 2.1 数据来源
主要来源于 **CommonCrawl** 和 **Github**。

### 2.2 过滤策略 (按文件层级)
为了确保代码质量，对原始数据执行了严格的启发式规则过滤：

*   **行长限制**：过滤掉平均行长超过 100 字符，或最大行长超过 1000 字符的文件（通常为被压缩或自动生成的代码）。
*   **字符占比**：剔除字母字符（Alphabetic characters）占比低于 25% 的文件。
*   **特定格式过滤**：
    *   **XML**：移除文件头包含 `<?xml version=` 的文件。
    *   **HTML**：保留可见文本占比至少 20% 且长度不少于 100 字符的文件（过滤文本过少或过短的网页）。
    *   **JSON/YAML**：仅保留字符数在 50 至 5000 之间的文件（避免过大或过小的数据）。

**处理结果**：
最终获得 **821B Token 纯代码数据** 以及 **185B Token 代码相关数据**。

### 2.3 后处理
*   **近端去重 (Near-deduplication)**：进一步去除重复内容。
*   **上下文保留**：保留代码相关的文本文件，如 Markdown 文档和 GitHub Issues，以增强模型对代码上下文的理解。

## 3. 代码与数学相关网页文本处理 (Web Texts)

### 3.1 处理流程 (Pipeline)
为了从海量网页中挖掘高质量的代码和数学文本，采用了“种子-分类-扩展”的三步法：

1.  **构建种子数据集 (Seed Corpus)**：
    *   收集高质量垂直站点的页面，包括 StackOverflow、PyTorch 官方文档、以及 StackExchange 等数学网站。
2.  **训练分类模型**：
    *   基于种子数据训练一个 **fastText** 模型。
    *   **Tokenizer 选择**：使用 BPE (Byte-Pair Encoding) 分词，以便更好地处理中文等非空格分词语言。
    *   利用该模型从 CommonCrawl 中召回更多与种子数据相似的代码或数学相关网页。
3.  **基于域名的扩展与人工校验**：
    *   统计各域名下被识别为 Math/Code 的页面比例。
    *   若某域名下超过 10% 的数据被分类器命中，则对该域名下的 URL 进行人工标注与核查。
    *   通过核查的 URL 数据将被正式归入代码或数学语料库。

### 3.2 处理结果
*   **CommonCrawl 来源**：
    *   代码相关 Token：**70B**
    *   数学相关 Token：**221B**
*   **GitHub 来源**：
    *   执行类似流程后获得 Token：**94B**
