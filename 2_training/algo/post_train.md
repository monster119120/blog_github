# 后训练（Post-training）工作规划

## 一、技术现状

### 1. 当前 Benchmark 效果概览

| 领域 | 平均表现 | 表现分析 | 关键问题 |
| :--- | :--- | :--- | :--- |
| **知识类** | 95.2% | 整体优秀 | GPQA-Diamond 得分仅 67.8%，专业领域知识有待加强 |
| **安全类** | 99.3% | 表现优异 | 评估维度相对单一，需进一步扩充 |
| **指令遵循** | 98.0% | 表现优秀 | 仅覆盖 IFEval，评估全面性不足 |
| **代码类** | 76.0% | **薄弱环节** | Live_code_bench 仅 51.3%，实际编程实战能力不足 |
| **数学类** | 78.5% | **薄弱环节** | 高难度数学问题（如 AIME）表现较差 |
| **推理类** | 100.4% | 表现优异 | 已达到理想水平 |

### 2. 核心问题分析

#### A. 训练方法瓶颈
*   **Logits 蒸馏**：
    *   ✅ **优势**：整体效果好，回答风格佳，内容详细。
    *   ❌ **劣势**：缺乏灵活性，能力上限受限于 Teacher 模型（V3），存在**自我认知偏差**（如错误地自称为 DeepSeek-V3 而非 Lightning）。
*   **SFT 方法**：
    *   ✅ **优势**：Benchmark 分数高，灵活性强，易于调整自我认知。
    *   ❌ **劣势**：当前版本回答偏短，人工评估（Human Eval）体验欠佳。
*   **训练流程**：
    *   流程较为繁琐：`4k off-policy 蒸馏 (450w+ 数据)` → `长文 off-policy (10w)` → `on-policy` → `DPO`。
    *   **数据冗余**：470 万指令数量庞大，且数据分布更偏向于蒸馏拟合，而非针对 SFT 优化。

#### B. 基础设施不完善
*   **RL 基建**：尚未完全搭建完毕，训练效率有待提升。
*   **框架依赖**：目前基于 VERL 框架的开发仍在进行中。

#### C. 评估体系不完整
*   缺少对**创意写作、Agent、SWE（软件工程）**等重要能力的评测。
*   部分核心能力（如指令遵循）的评测维度单一，缺乏多角度验证。

---

## 二、Post-training 训练范式调研

总体来看，当前主流范式均为 **“先 SFT 后 RL”**。RL 阶段主要分为“分治式”和“混合式”两种路径。

### 1. RL 路径对比

*   **分治式 RL (Task-Specific RL)**
    *   **特点**：针对数学、代码等不同能力域，分别训练专门的模型。
    *   **优势**：训练目标明确，容易在单项能力上达到最优。
    *   **劣势**：各模型间能力可能冲突，后期融合需要多轮迭代平衡。
*   **统一混合 RL (Unified Multi-Task RL)**
    *   **特点**：在统一框架下同时优化多项能力。
    *   **优势**：能力发展更均衡，能有效避免灾难性遗忘。
    *   **挑战**：奖励函数（Reward Function）设计复杂，训练难度更高。

### 2. 训练任务分类
1.  **推理密集型**：数学推理（GSM8K, MATH）、科学推理、逻辑链条构建。
2.  **代码生成与理解**：代码生成（HumanEval）、代码解释与 Debug、算法设计。
3.  **Agent 与工具使用**：API 调用、多步规划、环境交互（SWE-bench）。
4.  **通用交互能力**：指令遵循、角色扮演、多轮对话、RAG 知识应用。
5.  **安全与对齐**：有害内容拒绝、价值观对齐、诚实性（避免幻觉）。

### 3. 训练顺序策略
**主流顺序：推理优先 → 通用能力**
> `数学/代码推理` → `Agent 能力` → `通用指令` → `安全对齐`

*   **原理**：推理能力是基础认知能力；避免通用训练稀释专项推理能力；安全对齐置于最后以确保最终产品的安全性。

---

## 三、Lightning 后训练近期工作规划

### 1. SFT / Off-policy 蒸馏阶段优化

| 具体任务 | 预期效果 | 优先级 |
| :--- | :--- | :--- |
| **自我认知问题修复**<br>1. 调小学习率。<br>2. 调小 SFT 学习率并混合少量通用数据。<br>3. 下一版本蒸馏剔除自我认知数据，防止干扰。<br>4. 使用 DPO 进行针对性修复。 | 1. 自我认知能力达到可用标准（人工测评）。<br>2. 其他核心能力指标不下降。 | **P0** |
| **DS-V3.1 蒸馏样本构建**<br>1. 申请 DS-V3.1 API，Dump 样本。<br>2. 实验 `<think>` 标签内容的混合比例及 Dump 方式，确定最优配比。 | 升级 SFT 样本质量，利用 Chain-of-Thought 提升模型表现。 | **P0** |
| **指令数据优化与精简**<br>1. 对 470w 指令进行质量分析与筛选，提高多样性及业务数据占比。<br>2. 数据合成：合成高质量复杂指令集。 | 1. 产出 100-150w 精选指令集。<br>2. 合成 50w 高质量复杂指令集。 | **P1** |
| **训练损失优化探索 (SFT Loss vs KD Loss)**<br>1. 强项领域（如自我认知、超越 Teacher 的领域）使用 Completion + SFT Loss。<br>2. 其他领域维持 KD Loss。<br>3. 探索两者平衡方案。 | 1. 找到更优的训练策略。<br>2. 实现 Benchmark 指标提升与 V3 风格的平衡。 | **P2** |

### 2. 可验证问题（数学/代码/推理）- RL 优化

| 具体任务 | 预期效果 | 优先级 |
| :--- | :--- | :--- |
| **RL 通路搭建**<br>在 VERL 框架上跑通 Lightning 模型的 **GRPO** 训练。 | 1. 训练 Loss & Reward 曲线正常。<br>2. 数学、代码能力有明显提升。 | **P0** |
| **专项训练数据优化**<br>对数学、学科、代码等开源数据进行收集、筛选，并按难易度分级组织。 | 1. 确定一份最优的专项数据集。<br>2. 提升相关 Benchmark 分数。 | **P1** |
| **训练策略优化**<br>1. 测试 DAPO、GSPO 等前沿算法。<br>2. 优化 Rollout 采样方法。 | 1. 确定最优训练策略。<br>2. 进一步提升 Benchmark 分数。 | **P1** |

### 3. 主观问题（通用/风格/安全）- RL 优化

| 具体任务 | 预期效果 | 优先级 |
| :--- | :--- | :--- |
| **RL 通路搭建**<br>基于 Skywork RM 在 VERL 上跑通 Lightning 模型的 **RLHF (PPO)** 训练。 | 1. 训练 Loss & Reward 曲线正常。<br>2. 验证通用对话效果。 | **P0** |
| **通用训练数据优化**<br>1. 收集开源数据，进行质量筛选，确定领域配比。<br>2. 收集业务侧数据。 | 1. 确定一份最优通用数据集。<br>2. 提升开源 Benchmark 及业务指标。 | **P1** |
| **奖励模型 (Reward Model) 选型**<br>明确最优奖励信号来源：开源 Skywork / LLM-as-judge / 自研训练 RM。 | 1. 针对不同领域确定最优奖励信号。<br>2. 提升综合指标。 | **P1** |

### 4. Agent 能力建设

| 具体任务 | 预期效果 | 优先级 |
| :--- | :--- | :--- |
| **Agent 评测集调研**<br>构建 Agent 评估体系，对当前版本进行摸底。 | 建立 Agent 能力基线。 | **P0** |
| **Agent SFT 数据收集 & 初步实验**<br>收集 Function Call 训练数据，按照 V3.1 Template 处理并实验。 | 初步提升模型工具调用能力，且不影响其他能力。 | **P0** |
| **RL 通路搭建**<br>1. 基于 VERL 跑通 Lightning 模型 Agentic RL 训练。<br>2. 明确架构优化需求。 | 提升 Agent Benchmark 能力。 | **P1** |

### 5. RL / On-policy 训练框架加速

| 具体任务 | 预期效果 | 优先级 |
| :--- | :--- | :--- |
| **Partial Rollout 实现** | 训练加速显著提升 | **P0** |
| **One-step Off-policy 优化** | 训练加速显著提升 | **P0** |
| **异步推理框架开发** | 推理效率提升 | **P0** |
| **FlashRL 等技术调研** | 探索极致加速方案 | **P1** |
| **Rollout 与 Teacher Knowledge 并发处理** | 提升数据生成效率 | **P1** |
| **Update_actor 优化 (Megatron 计算效率)** | 提升计算利用率 | **P1** |

### 6. Benchmark 扩充与评测

| 具体任务 | 预期效果 | 优先级 |
| :--- | :--- | :--- |
| **Benchmark 体系补充**<br>1. 补充创意写作、指令遵循、回答风格相关 Benchmark。<br>2. 建立 AI 搜索多轮 Session 标准。<br>3. 完善 DQA 自动评估手册。 | 建立更全面、立体的模型能力评估体系。 | **P0** |